{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports \nThis project is implemented using tensorflow vesion 2.7.0 and numpy version 1.19.5 ","metadata":{"id":"4uN9mdMlZwcK"}},{"cell_type":"code","source":"import sys \nimport random \nimport numpy as np \nimport tensorflow as tf\nfrom tensorflow.keras import layers, models  \n\nprint(tf.__version__)\nprint(np.__version__)","metadata":{"id":"jx7UCJdHZ2_R","outputId":"2d8ce212-9794-4071-9496-3f7461da9ef2","execution":{"iopub.status.busy":"2022-01-17T06:19:08.569765Z","iopub.execute_input":"2022-01-17T06:19:08.570060Z","iopub.status.idle":"2022-01-17T06:19:13.948540Z","shell.execute_reply.started":"2022-01-17T06:19:08.570031Z","shell.execute_reply":"2022-01-17T06:19:13.947721Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing \nOnce we load our data we will create a vocabulary which is composed unique characters from the data. One thing to note here is that these exploits will include a number of special characters and we have to keep them in there ans sometimes even upper case letter. They are part of the more diversified exploit content. We just load the data and vectorize its content\n### Vectorization\nInorder to feed our data we have to convert the string form of the data into numeric tensors or numeric vectors. for this we can use StringLookup layer form tensorflow or do it manually using simple dictionary operations. Inorder to convert generated numeric IDs into human readable form we can use the StringLookup layer of tensorflow with invert parameter set to True.      \n\n```\n# without tensorflow \nchars_to_indices = tf.keras.layers.StringLookup(\n    vocabulary=list(vocabulary), mask_token=None)\n\nindices_to_chars = tf.keras.layers.StringLookup(\n    vocabulary=chars_to_indices.get_vocabulary(), invert=True, mask_token=None\n)\n```\n\n# Character Mappings \nmapping character is just giving a unique id to a character from the data we just loaded ofcourse for those which are unique. This is important later when we feed the data into our model. The model will use the ids to map each character thats coming through the sequence.  ","metadata":{"id":"sWylYINJa9fD"}},{"cell_type":"code","source":"# Read our data \ndata = \"\"\nwith open(\"../input/payloads/payloads.txt\", 'r', encoding='utf-8') as f:\n  data += f.read()\n\n# Create  our vocabulary \nvocabulary = sorted(list(set(data)))\nprint(f\"Length our vocabulary {len(vocabulary)}\")\n\n# Create mapping chars to indices and reverse indices to chars \nchars_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\nindices_to_chars = dict((i, c) for i, c in enumerate(vocabulary))","metadata":{"id":"2j2Urs2ecH0A","outputId":"f05e6495-7e35-4dc8-e437-ef6e03709e50","execution":{"iopub.status.busy":"2022-01-17T06:18:46.386007Z","iopub.execute_input":"2022-01-17T06:18:46.386933Z","iopub.status.idle":"2022-01-17T06:18:46.436056Z","shell.execute_reply.started":"2022-01-17T06:18:46.386821Z","shell.execute_reply":"2022-01-17T06:18:46.435169Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Split the data into training examples and targets \n\nThe data is splitted into smaller sequences of a fixed length which makes all input sequences the same length and one character shifted to the right. The shifted character will be the target value which we want to predict.\n\n```\n# with Tensorflow\n# get the mapping of all characters from our input file \nvector_data = chars_to_indices(tf.strings.unicode_split(data, 'UTF-8')) \n\nvector_dataset = tf.data.Dataset.from_tensor_slices(vector_data)\n```\n\n","metadata":{"id":"Xq-m-p8og4gO"}},{"cell_type":"markdown","source":"# Create a batch of characters \nthe batch method of tensorflow creates a sequence of characters with desired length in this case the sequence length is set to 200\n\n\n\n```\n# Tensorflow implementation \n\nseq_length = 200\nsequences = vector_dataset.batch(seq_length+1, drop_remainder=True)\n\n# utility function to split input sequence into sample and target \ndef split_sequence(sequence):\n  in_seq = sequence[:-1]\n  target_seq = sequence[1:]\n  return in_seq, target_seq\n\n# then create dataset of input sample and target over the entire sequence \nnew_dataset = sequences.map(split_sequence)\n\n```\n\n","metadata":{"id":"DQ6g-qQwoRc7"}},{"cell_type":"code","source":"seq_length = 200\nstep = 3\nsequences = []\nnext_chars = []\nfor i in range(0, len(data) - seq_length, step):\n    sequences.append(data[i: i + seq_length])\n    next_chars.append(data[i + seq_length])","metadata":{"id":"oIr43TCUovxW","execution":{"iopub.status.busy":"2022-01-17T06:18:58.203459Z","iopub.execute_input":"2022-01-17T06:18:58.203753Z","iopub.status.idle":"2022-01-17T06:18:58.268862Z","shell.execute_reply.started":"2022-01-17T06:18:58.203725Z","shell.execute_reply":"2022-01-17T06:18:58.267944Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Create One-Hot-Encoding matrix for X and Y\n\nAfter the sequences are created we want to make sure that these sequences are in the right form for our model by encoding them. One-Hot-Encoding scheme is used where each character i from our vocabulary is represented in the form of binary 0 and 1. To denore the i-th word from our vector, the value at the i-th element is set to 1 where all other values are set to 0.\n\nOne thing to note here the X stores encode form of a sequence which is shifted by one and the y store the shifted character to represent the next incoming character from the input X sequence. As an example \n\n\n\n```\n+----------------------------------------------+-----+\n| [', ;, a, =, p, r, o, m, p, t, a, (, ), / ]  | [/] |\n+----------------------------------------------+-----+\n\n# or as a hello\n+--------------+-------+\n|      X       |   Y   |\n+--------------+-------+\n| [h, e, l, l] | [o]   |\n| [e, l, l, o] | [ ]   |\n| [l, l, o,  ] | [i]   |\n| [l, o,  , i] | [n]   |\n| ...          | ...   |\n+--------------+-------+\n```\n\n","metadata":{"id":"KxDchaosohHO"}},{"cell_type":"code","source":"x = np.zeros((len(sequences), seq_length, len(vocabulary)), dtype=np.bool)\ny = np.zeros((len(sequences), len(vocabulary)), dtype=np.bool)\nfor i, sequence in enumerate(sequences):\n    for t, char in enumerate(sequence):\n        x[i, t, chars_to_indices[char]] = 1\n    y[i, chars_to_indices[next_chars[i]]] = 1\n\n# convert the integer values into floating point\nx = np.asarray(x).astype('float32')\ny = np.asarray(y).astype('float32')","metadata":{"id":"mzWLYqxuoonl","execution":{"iopub.status.busy":"2022-01-17T06:19:15.767302Z","iopub.execute_input":"2022-01-17T06:19:15.767602Z","iopub.status.idle":"2022-01-17T06:19:30.933816Z","shell.execute_reply.started":"2022-01-17T06:19:15.767573Z","shell.execute_reply":"2022-01-17T06:19:30.931018Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Validation set \nThe validation set will be used to validate if the training went well like expected, mainly used for testing weather our model overfits or not.","metadata":{"id":"YW-mBA00Sjeb"}},{"cell_type":"code","source":"# split our data into training set and validation set  \nx_val = x[:10000]\nx_train = x[10000:]\ny_val = y[:10000]\ny_train = y[10000:] ","metadata":{"id":"tJ6VWDaFsrts","execution":{"iopub.status.busy":"2022-01-17T06:19:36.662491Z","iopub.execute_input":"2022-01-17T06:19:36.663432Z","iopub.status.idle":"2022-01-17T06:19:36.671932Z","shell.execute_reply.started":"2022-01-17T06:19:36.663361Z","shell.execute_reply":"2022-01-17T06:19:36.670630Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Building the model \nThe two functions build and compile two different model with the same architecture. The build_rnn_model builds Simple RNN model with two Dropout layers and 1 Dense layer as an output. the activation function used is softmax because the output is multinomial probability distribution. the loss function is categorical_crossentropy and optimizer rmsprop. both function define the same architecture execpt the first function use SimpleRNN model and the second function defines LSTM model. This is done purposefully to study the performance of each model and do comparision between them.  ","metadata":{"id":"C9U__dVurWu4"}},{"cell_type":"code","source":"vocab_length = len(vocabulary) # 116\nembedding_dimension = 256 \nunits = 128\n\ndef build_rnn_model():\n  model = models.Sequential()\n  model.add(layers.SimpleRNN(units, input_shape=(seq_length, vocab_length), return_sequences=True))\n  model.add(layers.Dropout(0.2))\n  model.add(layers.SimpleRNN(units))\n  model.add(layers.Dropout(0.2))\n  model.add(layers.Dense(vocab_length, activation='softmax'))\n  model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n  return model \n\n\ndef build_lstm_model():\n  model = models.Sequential()\n  model.add(layers.LSTM(units, input_shape=(seq_length, vocab_length), return_sequences=True))\n  model.add(layers.Dropout(0.2))\n  model.add(layers.LSTM(units))\n  model.add(layers.Dropout(0.2))\n  model.add(layers.Dense(vocab_length, activation='softmax'))\n  model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n  return model","metadata":{"id":"5v0tgpY3qgoT","execution":{"iopub.status.busy":"2022-01-17T06:19:39.164955Z","iopub.execute_input":"2022-01-17T06:19:39.165674Z","iopub.status.idle":"2022-01-17T06:19:39.175894Z","shell.execute_reply.started":"2022-01-17T06:19:39.165638Z","shell.execute_reply":"2022-01-17T06:19:39.174970Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"rnn_model = build_rnn_model()\nrnn_model.summary()","metadata":{"id":"Tdz-6PAPtksU","outputId":"4c4ca0ae-1e40-4e34-a54f-1f0f80d08abe","execution":{"iopub.status.busy":"2022-01-17T06:19:44.859741Z","iopub.execute_input":"2022-01-17T06:19:44.860231Z","iopub.status.idle":"2022-01-17T06:19:45.027626Z","shell.execute_reply.started":"2022-01-17T06:19:44.860185Z","shell.execute_reply":"2022-01-17T06:19:45.026769Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"lstm_model = build_lstm_model()\nlstm_model.summary()","metadata":{"id":"bayUbDKoYcsK","outputId":"ad0781b2-1f60-4cea-e96a-4e6a3b73e4ba","execution":{"iopub.status.busy":"2022-01-17T06:19:47.469258Z","iopub.execute_input":"2022-01-17T06:19:47.469830Z","iopub.status.idle":"2022-01-17T06:19:48.025915Z","shell.execute_reply.started":"2022-01-17T06:19:47.469778Z","shell.execute_reply":"2022-01-17T06:19:48.025009Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions \nThe sample function and SampleExploit class are used to sample an index from a probability array and print out predictions from our model after each epoch. This will also give us a good idea if our model is overfitting or not since we can see what each epoch generated code snippet looks like.   ","metadata":{"id":"0iL4DnUfYQzz"}},{"cell_type":"code","source":"def sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n\nclass SampleExploit(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, batch, logs={}):\n        start_index = random.randint(0, len(data) - seq_length - 1)\n\n        for diversity in [0.5, 1.2]:\n            generated = ''\n            sentence = data[start_index: start_index + seq_length]\n            generated += sentence\n            sys.stdout.write(generated)\n            for i in range(200):\n                x_pred = np.zeros((1, seq_length, len(vocabulary)))\n                for t, char in enumerate(sentence):\n                    x_pred[0, t, chars_to_indices[char]] = 1.\n                preds = rnn_model.predict(x_pred, verbose=0)[0]\n                next_index = sample(preds, diversity)\n                next_char = indices_to_chars[next_index]\n                generated += next_char\n                sentence = sentence[1:] + next_char\n                sys.stdout.write(next_char)\n                sys.stdout.flush()\n            print()\n\n","metadata":{"id":"EdSBBioOtmpG","execution":{"iopub.status.busy":"2022-01-17T06:19:52.015503Z","iopub.execute_input":"2022-01-17T06:19:52.016373Z","iopub.status.idle":"2022-01-17T06:19:52.036254Z","shell.execute_reply.started":"2022-01-17T06:19:52.016322Z","shell.execute_reply":"2022-01-17T06:19:52.035295Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Training Model \nBoth Simple RNN model and LSTM are trained with the same parameters and validation data. The epochs are set to 20 and batch_size 80 with validation dataset x_val and y_val. The result of each epoch is stored inside a dictionary called history which we will use later for plotting graphs.  ","metadata":{"id":"dK9HYUAaYV4l"}},{"cell_type":"code","source":"import os \n\nrnn_training_checkpoint_path = \"rnn_training_1/cp.ckpt\"\nlstm_training_checkpoint_path = \"lstm_training_1/cp.ckpt\"\ngru_training_checkpoint_path = \"gru_training_1/cp.ckpt\"\n\nrnn_checkpoint_dir = os.path.dirname(rnn_training_checkpoint_path)\nlstm_checkpoint_dir = os.path.dirname(lstm_training_checkpoint_path)\ngru_checkpoint_dir = os.path.dirname(gru_training_checkpoint_path)\n\n\n# Create a callback that saves the model's weights\nrnn_cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=rnn_training_checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\nlstm_cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=lstm_training_checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\ngru_cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=gru_training_checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","metadata":{"id":"XhSsfC2H-C9w","execution":{"iopub.status.busy":"2022-01-17T06:19:55.575073Z","iopub.execute_input":"2022-01-17T06:19:55.575432Z","iopub.status.idle":"2022-01-17T06:19:55.582942Z","shell.execute_reply.started":"2022-01-17T06:19:55.575398Z","shell.execute_reply":"2022-01-17T06:19:55.581774Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Run our rnn model with 20 epochs and batch_size of 80\nEPOCHS = 20 \nrnn_history = rnn_model.fit(x_train, y_train, \n                        epochs=EPOCHS, \n                        batch_size=80, \n                        callbacks=[SampleExploit(), rnn_cp_callback],\n                        validation_data=(x_val, y_val))","metadata":{"id":"zhJb6bI8qQAl","outputId":"b6f6dd42-cd7b-44c4-a4bb-8679d85ac3a6","execution":{"iopub.status.busy":"2022-01-17T06:20:01.083927Z","iopub.execute_input":"2022-01-17T06:20:01.084264Z","iopub.status.idle":"2022-01-17T07:09:21.556058Z","shell.execute_reply.started":"2022-01-17T06:20:01.084229Z","shell.execute_reply":"2022-01-17T07:09:21.555092Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Run our lstm model with 20 epochs and batch_size of 80\nEPOCHS = 20 \nlstm_history = lstm_model.fit(x_train, y_train \n                         ,epochs=EPOCHS, \n                         batch_size=80, \n                         callbacks=[SampleExploit(), lstm_cp_callback],\n                         validation_data=(x_val, y_val))","metadata":{"id":"gZ-z71y5qkbY","outputId":"1f0489f0-9937-4bff-f1cd-ca49cbb95e73","execution":{"iopub.status.busy":"2022-01-16T18:09:18.914938Z","iopub.execute_input":"2022-01-16T18:09:18.915254Z","iopub.status.idle":"2022-01-16T18:29:52.272234Z","shell.execute_reply.started":"2022-01-16T18:09:18.915221Z","shell.execute_reply":"2022-01-16T18:29:52.268943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Validation loss plot \nFor the Simple RNN the model starts to overfit after the 9-th epochs from these information we can use the EarlyStopping to minize overfitting. \nAs for LSTM the overfitting starts from the 12-th epoch and apply Early stopping and retrain the model, meaning retrain the model using fewer number of epochs.  ","metadata":{"id":"ny3SofS3Wz14"}},{"cell_type":"code","source":" \n# plot the training and validation loss for the RNN model\nimport matplotlib.pyplot as plt\n\nloss_values = rnn_history.history['loss']\nvalidation_loss = rnn_history.history['val_loss']\nepochs = range(1, len(loss_values)+1)\n\n# SimpleRNN validation loss plot\nplt.plot(epochs, loss_values, 'bo', label='Training Loss')\nplt.plot(epochs, validation_loss, 'b', label='Validation Loss')\nplt.title(\"Training and Validation Loss for Simple RNN\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"id":"xRi6XqvQzrRd","outputId":"0a359712-aaee-4c97-e434-cb2315698d68","execution":{"iopub.status.busy":"2022-01-17T07:13:12.662968Z","iopub.execute_input":"2022-01-17T07:13:12.663345Z","iopub.status.idle":"2022-01-17T07:13:12.957287Z","shell.execute_reply.started":"2022-01-17T07:13:12.663305Z","shell.execute_reply":"2022-01-17T07:13:12.956625Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# plot the training and validation loss for the RNN model\nimport matplotlib.pyplot as plt\n\n# LSTM validation loss plot \nlstm_loss = lstm_history.history['loss']\nlstm_validation_loss = lstm_history.history['val_loss']\nepochs = range(1, len(lstm_loss)+1)\n\n# LSTM validation loss plot \nplt.plot(epochs, lstm_loss, 'bo', label=\"Training Loss\")\nplt.plot(epochs, lstm_validation_loss, 'b', label='Validation Loss')\nplt.title(\"Training and Validation Loss for LSTM\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T18:33:09.697146Z","iopub.execute_input":"2022-01-16T18:33:09.697488Z","iopub.status.idle":"2022-01-16T18:33:10.054572Z","shell.execute_reply.started":"2022-01-16T18:33:09.697454Z","shell.execute_reply":"2022-01-16T18:33:10.053494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrain the model Early stoping \nHere the both models are trained with fewer number of epochs to avoid overfitting. ","metadata":{"id":"XuB-38BwPV57"}},{"cell_type":"code","source":"# Create fresh Simple RNN model and retrain it will less number of epochs \nrnn_model = build_rnn_model()\nrnn_model.load_weights(rnn_training_checkpoint_path)\nrnn_model.fit(x_train, y_train,\n              epochs=15,\n              batch_size=80,\n              callbacks=[SampleExploit(), rnn_cp_callback],\n              validation_data=(x_val, y_val))","metadata":{"id":"CCaDYQqbPab6","outputId":"2ad6267e-344b-452a-f436-e46dee62efca","execution":{"iopub.status.busy":"2022-01-16T18:33:38.694292Z","iopub.execute_input":"2022-01-16T18:33:38.694633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create fresh Simple LSTM model and retrain it will less number of epochs \nlstm_model = build_lstm_model()\nlstm_model.load_weights(lstm_training_checkpoint_path)\nlstm_model.fit(x_train, y_train, \n               epochs=15,\n               batch_size=80,\n               callbacks=[SampleExploit(), lstm_cp_callback],\n               validation_data=(x_val, y_val))","metadata":{"id":"kWtutOWiQJ9v","outputId":"ca7ee545-4319-48c0-eaab-2eecf7f8975e","execution":{"iopub.status.busy":"2022-01-16T18:36:15.775357Z","iopub.execute_input":"2022-01-16T18:36:15.776194Z","iopub.status.idle":"2022-01-16T18:51:50.253366Z","shell.execute_reply.started":"2022-01-16T18:36:15.776156Z","shell.execute_reply":"2022-01-16T18:51:50.24967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GRU model\nLet's build GRU model and train it with our trining data. GRUs are less complicated compared to LSTM and RNN models and much faster. Later they can be compared with the prior two models implemented above. The model has the same architecture except here GRU layer is used instead of SimpleRNN and LSTM.","metadata":{"id":"-LS_4E5XY4YN"}},{"cell_type":"code","source":"def build_gru_model():\n  model = models.Sequential()\n  model.add(layers.GRU(units, input_shape=(seq_length, vocab_length), return_sequences=True))\n  model.add(layers.Dropout(0.2))\n  model.add(layers.GRU(units))\n  model.add(layers.Dropout(0.2))\n  model.add(layers.Dense(vocab_length, activation='softmax'))\n  model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n  return model","metadata":{"id":"CbANZej1ZSGj","execution":{"iopub.status.busy":"2022-01-16T19:28:05.60406Z","iopub.execute_input":"2022-01-16T19:28:05.605163Z","iopub.status.idle":"2022-01-16T19:28:05.612942Z","shell.execute_reply.started":"2022-01-16T19:28:05.605126Z","shell.execute_reply":"2022-01-16T19:28:05.611548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru_model = build_gru_model()\ngru_model.summary()","metadata":{"id":"MP6s-niFZpfP","outputId":"04fbd967-f28b-49d0-99c6-6c1a4c5a6daa","execution":{"iopub.status.busy":"2022-01-16T19:28:08.038094Z","iopub.execute_input":"2022-01-16T19:28:08.03881Z","iopub.status.idle":"2022-01-16T19:28:08.558897Z","shell.execute_reply.started":"2022-01-16T19:28:08.038763Z","shell.execute_reply":"2022-01-16T19:28:08.556376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru_history = gru_model.fit(x_train, y_train, \n                epochs=20,\n                batch_size=80,\n                callbacks=[SampleExploit(), gru_cp_callback],\n                validation_data=(x_val, y_val))","metadata":{"id":"KOmj7xxDBnzN","outputId":"c3d8b7e9-ba06-4d1e-adad-4d74aec7984c","execution":{"iopub.status.busy":"2022-01-16T19:04:27.946508Z","iopub.execute_input":"2022-01-16T19:04:27.947116Z","iopub.status.idle":"2022-01-16T19:25:01.988738Z","shell.execute_reply.started":"2022-01-16T19:04:27.947058Z","shell.execute_reply":"2022-01-16T19:25:01.985564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# GRU validation loss plot \ngru_loss = gru_history.history['loss']\ngru_validation_loss = gru_history.history['val_loss']\nepochs = range(1, len(gru_loss)+1)\n\n# GRU validation loss plot \nplt.plot(epochs, gru_loss, 'bo', label=\"Training Loss\")\nplt.plot(epochs, gru_validation_loss, 'b', label='Validation Loss')\nplt.title(\"Training and Validation Loss for GRU\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"id":"U749imX6UrOx","execution":{"iopub.status.busy":"2022-01-16T19:25:02.63873Z","iopub.execute_input":"2022-01-16T19:25:02.638952Z","iopub.status.idle":"2022-01-16T19:25:02.904806Z","shell.execute_reply.started":"2022-01-16T19:25:02.638923Z","shell.execute_reply":"2022-01-16T19:25:02.903524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrain the model with less number epochs \ngru_model = build_gru_model()\ngru_model.load_weights(gru_training_checkpoint_path)\ngru_model.fit(x_train, y_train,\n              epochs=15,\n              batch_size=80,\n              callbacks=[SampleExploit()],\n              validation_data=(x_val, y_val))","metadata":{"id":"2VjqUtwvVJkH","execution":{"iopub.status.busy":"2022-01-16T19:28:35.68722Z","iopub.execute_input":"2022-01-16T19:28:35.687748Z","iopub.status.idle":"2022-01-16T19:44:10.352515Z","shell.execute_reply.started":"2022-01-16T19:28:35.687696Z","shell.execute_reply":"2022-01-16T19:44:10.349309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_length = 200\nstep = 3\nsequences = []\nnext_chars = []\n\ntest_data = \"\"\n\nwith open(\"../input/test-data/test_payloads.txt\", 'r', encoding='utf-8') as f:\n  test_data += f.read()\n\ndef process_test_data(data):\n  for i in range(0, len(data) - seq_length, step):\n    sequences.append(data[i: i + seq_length])\n    next_chars.append(data[i + seq_length])\n  x_test = np.zeros((len(sequences), seq_length, len(vocabulary)), dtype=np.bool)\n  y_test = np.zeros((len(sequences), len(vocabulary)), dtype=np.bool)\n  for i, sequence in enumerate(sequences):\n      for t, char in enumerate(sequence):\n          x_test[i, t, chars_to_indices[char]] = 1\n      y_test[i, chars_to_indices[next_chars[i]]] = 1\n\n  # convert the integer values into floating point\n  x_test = np.asarray(x_test).astype('float32')\n  y_test = np.asarray(y_test).astype('float32')\n  return x_test, y_test\n\n\nx_test, y_test = process_test_data(test_data)\nx_test.shape","metadata":{"id":"u3vQAgEd4k6-","execution":{"iopub.status.busy":"2022-01-16T04:15:50.784384Z","iopub.execute_input":"2022-01-16T04:15:50.78466Z","iopub.status.idle":"2022-01-16T04:15:51.120078Z","shell.execute_reply.started":"2022-01-16T04:15:50.784628Z","shell.execute_reply":"2022-01-16T04:15:51.119139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrain the model with less number epochs \ngru_model = build_gru_model()\ngru_model.load_weights(gru_training_checkpoint_path)\ngru_model.fit(x_test, y_test,\n              epochs=15,\n              batch_size=80,\n              callbacks=[SampleExploit()])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:15:55.050052Z","iopub.execute_input":"2022-01-16T04:15:55.050906Z","iopub.status.idle":"2022-01-16T04:23:30.196876Z","shell.execute_reply.started":"2022-01-16T04:15:55.050865Z","shell.execute_reply":"2022-01-16T04:23:30.196008Z"},"trusted":true},"execution_count":null,"outputs":[]}]}